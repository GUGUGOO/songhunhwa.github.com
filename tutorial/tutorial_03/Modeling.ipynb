{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('precision', 4)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "1. Read the dataset (after extracting)\n",
    "2. Preprocessing\n",
    "3. EDA\n",
    "4. Modeling\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset\n",
    "- 데이터셋: 유저별(iduser) 문서 사용행동에 대한 횟수와 그룹 특성(group)를 처리한 데이터\n",
    "    - 3개의 테이블 소스에서 Raw 데이터를 가공: ``` groupby(\"iduesr\").agg(count,,,sum,,,etc)``` 이후에 조인함\n",
    "    - 결제 타이밍 기준(유저마다 다름)으로 이전 30일의 행동 기준으로 데이터 추출\n",
    "- 간혹 csv를 불러올 때 unnamed 라는 컬럼이 자동으로 생성되므로, index_col=0 이라는 명령어를 통해 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"testset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Info.\n",
    "- ✭iduser: 식별값\n",
    "- mdutype: 중요x\n",
    "- ✭✭group: y, 결제(mdu) vs 비결제(sdu) 정보\n",
    "- ✭viewCount: 보기 횟수\n",
    "- ✭editCount: 편집 횟수\n",
    "- ✭shareCount: 공유 횟수\n",
    "- ✭searchCount: 검색 횟수\n",
    "- ✭coworkCount: 공동작업 횟수\n",
    "- add: 파일 추가 횟수\n",
    "- del: 파일 삭제 횟수\n",
    "- move: 파일 이동 횟수\n",
    "- rename: 파일명 변경 횟수\n",
    "- adddir: 폴더 생성\n",
    "- movedir: 폴더 이동\n",
    "- ✭✭visdays: 방문일수\n",
    "- ✭openCount: 열기 횟수\n",
    "- ✭saveCount: 저장 횟수\n",
    "- ✭exportCount: 내보내기 횟수\n",
    "- viewTraffic: 보기 용량(파일 사이즈)\n",
    "- editTraffic: 편집 용량\n",
    "- exportTraffic: 내보내기 용량\n",
    "- traffic: 전체 용량\n",
    "\n",
    "![img](po_detail.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas DF index 지정\n",
    "- 유저ID와 같은 유니크 값(primary key)를 인덱스로 지정하는 것이 편리함 (pandas 장점)\n",
    "- 그렇지 않으면, 추후 scaling 이나 모델 학습 등을 할때 매번 슬라이싱으로 처리해야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"iduser\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"mdutype\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in each cols\n",
    "df.isnull().sum().plot(kind='barh', color='darkblue', figsize=(10,6))\n",
    "\n",
    "plt.title(\"Missing value counts\")\n",
    "plt.grid(color='lightgrey', alpha=0.5, linestyle='--')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치 처리\n",
    "\n",
    "- 추후에는 협의를 통해 결측치 발새 이유 파악 및 예방에 노력을 기울이는 것이 필요\n",
    "- 결측치 처리 방법\n",
    "    - 가장 쉬운 방법은 Null이 포함 행을 모두 제거하는 것이다\n",
    "    - 사례(observation)이 많다면 이 방법을 사용하는 것이 가능하다\n",
    "    - 평균, 중앙치, 최빈치, 간단한 예측 모델활용해서 imputation\n",
    "\n",
    "    - 만약 샘플수가 충분하지 않을 경우, Pandas의 fillna() 명령어로 Null 값을 채우는 것이 가능하다. \n",
    "    - 연속형인 경우 Mean이나 Median을 이용하고 명목형인 경우 Mode(최빈치)나 classification 모델을 통해 Null 값을 대체할 수 있다.\n",
    "\n",
    "```python\n",
    "# Null 값을 median, mean으로 대체하는 코드 예제\n",
    "df.fillna(df.med())\n",
    "df.fillna(df.mean()) \n",
    "\n",
    "# Scikit-learn Imputation을 이용하여 명목변수의 Null 값을 Mode로 대체한 예제\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy='most_frequent', axis=0)\n",
    "df['X'] = imp.fit_transform(df['X'])\n",
    "\n",
    "# 참고: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 group 컬럼이 null 경우만 선택\n",
    "df[df['group'].isnull() == True].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visdays 컬럼이 null 경우만 선택\n",
    "df[df['visdays'].isnull() == True].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 열을 기준으로 dropna\n",
    "df1 = df.dropna(subset=['visdays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q) visiday, group 을 제외한 나머지 항목이 nan이면 drop 해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset 용 컬럼 설정\n",
    "mycols = df1.columns[1:].drop('visdays')\n",
    "\n",
    "# mycols 컬럼들을 기준으로 drop\n",
    "df2 = df1.dropna(subset=mycols, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['viewCount'].isnull() == True][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['add'].isnull() == True][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['openCount'].isnull() == True][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['traffic'].isnull() == True][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add, del ~ movedir 까지는 파일 management 관련 변수이므로 상대적 중요도 낮음\n",
    "- 변수 삭제를 고려해볼 수 있으나 우선 pass\n",
    "- 우선 메꾸고 추후 제거도 고려 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero로 imputaion\n",
    "df2 = df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간략한 기술 통계 확인\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Missing Value 처리 가이드\n",
    "\n",
    "- Missing Value 파악을 위해 `df.info()` 가장 처음에 이용\n",
    "- 만약 np.nan으로 적절히 missing value로 불러왔다면 info() 이용 가능하지만,\n",
    "- '', ' ' 이런식의 공백이나 다른 방식(.)으로 처리되어 있다면, 모두 repalce 처리해야함\n",
    "- `df.info()`를 실행했을 때, 누가봐도 float or int 인데 object(string)으로 되어 있다면 이런 사레가 포함될 가능성 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 dataframe 생성\n",
    "tt = df2[['group', 'viewCount']]\n",
    "\n",
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.nan 대신 ''로 수집된 경우\n",
    "tt.loc[10100037810674,'viewCount'] = ''\n",
    "tt.loc[10100036273719,'viewCount'] = ''\n",
    "\n",
    "# np.nan 대신 '.'로 수집된 경우\n",
    "tt.loc[10100034746743,'group'] = '. '\n",
    "tt.loc[10100016781863,'group'] = '. '\n",
    "\n",
    "# if missing is zero.......?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.info() # if continous var such as viewcount is object object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 큰 데이터셋에서는 찾는 경우는 정렬을 이용\n",
    "tt.sort_values(\"viewCount\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.query(\"viewCount == ''\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.sort_values(\"group\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.query(\"group == '.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백 제거\n",
    "tt['group'] = tt['group'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.query(\"group == '.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 변수에 공백이 있을 경우\n",
    "tt.columns = ['group ', ' viewCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip 함수 이용\n",
    "tt.columns = tt.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no float, replace with np.nan\n",
    "for i in tt.index:\n",
    "    if type(tt.loc[i, 'viewCount']) == float:\n",
    "        tt.loc[i, 'viewCount'] == tt.loc[i, 'viewCount']    \n",
    "    else:\n",
    "        tt.loc[i, 'viewCount'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace 이용 방식 (단 에러 값을 정확히 알고 있어야 함)\n",
    "tt['group'] = tt['group'].replace('.', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null with mean\n",
    "tt['viewCount'] = tt['viewCount'].fillna(tt.viewCount.mean())\n",
    "\n",
    "# fill null with mode\n",
    "tt['group'] = tt['group'].fillna(tt.group.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치를 처리할 때 고려할 점\n",
    "- 결측치를 처리할 경우에도 도메인 지식은 유용하게 사용된다.\n",
    "- 인적, 기계적 원인임이 판명되면, 협업자와 지속적으로 노력해 결측치를 **사전에 발생하지 않도록 조치**하는 것이 좋다\n",
    "- 수치형인 경우 의미상으로 0으로 메꾸는 것이 맞는지 아니면 평균이나 중앙치, 최빈치가 맞는지 정확히 판단해야 한다.\n",
    "    - 예를 들어 viewCount가 1이상인데, edit, export가 missing인 경우 (도메인 지식을 통해) 0으로 메꾸는 것이 가능하다.\n",
    "    - 왜냐하면, ViewCount가 다른 행동에 선행하는 개념이기 때문에 위와 같은 의사결정이 가능하다\n",
    "- 특히 **숫자 0과 null 과 같은 결측치는 완전히 다른 개념**이니 유의해야 한다.\n",
    "    - 0: -1과 1 사이의 가운데 값(숫자)임. '제로'라는 의미를 지니고 있음.\n",
    "    - null or nan: 미지의 값 (모름)\n",
    "- 만약 y label(위 샘플 데이터에서는 'group')에 결측치가 있다면 그냥 drop\\\n",
    "- pandas 결측치 관련 API: https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "- 참고 블로그: https://machinelearningmastery.com/handle-missing-data-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 이상치(Outlier) 처리 방법\n",
    "\n",
    "#### 0. 이상치란?\n",
    "\n",
    ">In statistics, an outlier is a data point that differs greatly from other values in a data set. Outliers are important to keep in mind when looking at pools of data because they can sometimes affect how the data is perceived on the whole.\n",
    "\n",
    "\n",
    "#### 1. 표준편차(standard deviation) 이용\n",
    "- 현재 분포에서 표준편차 기준 +3 이상이거나 -3 이하인 경우 극단치로 처리\n",
    "- 정규분포일 경우 유용\n",
    "\n",
    "#### 2. 표준점수(z-score) 이용\n",
    "- 평균을 0, 표준편차를 1로 맞춘후 표준편차 이용 (+-3 벗어난 경우 이상치로 판정)\n",
    "- 정규분포일 경우 유용\n",
    "\n",
    "![img](out_std.png)\n",
    "\n",
    "\n",
    "#### 3. IQR 방식\n",
    "- 75 percentile + IQR * 1.5 이상이거나 25 percentile + IQR * 1.5 이하인 경우 극단치로 처리\n",
    "- 정규분포 아닐 경우 robust한 편\n",
    "\n",
    "![img](out_iqr.png)\n",
    "\n",
    "#### 4. 기타 방법\n",
    "- Rule-based (ex, percentile 95% 이상이면 제거)\n",
    "- Binning (연속변인을 카테고리형으로 변환)\n",
    "\n",
    "#### 5. 발생이유\n",
    "- 입력 오류, 측정 오류, 고의성, 처리시 에러, 샘플링에러, 자연발생 등\n",
    "- 예방이 가장 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 처리 대상 데이터 (NA 제거한 dataframe)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자형 변수만 선택\n",
    "# df3 = df2.drop(\"group\", axis=1) \n",
    "df3 = df2._get_numeric_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "df3['viewCount'].plot(kind='box', ax=ax[0, 0], figsize=(5, 6));\n",
    "df3['editCount'].plot(kind='box', ax=ax[0, 1], figsize=(5, 6));\n",
    "df3['shareCount'].plot(kind='box', ax=ax[1, 0], figsize=(5, 6));\n",
    "df3['searchCount'].plot(kind='box', ax=ax[1, 1], figsize=(5, 6));\n",
    "\n",
    "plt.title(\"Boxplot\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.plot(kind='scatter', x='viewCount', y='editCount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준편차 이용 (6-sigma): 각 컬럼별 표준편차에서 +- std 벗어난 경우 제외\n",
    "def std_based_outlier(df):\n",
    "\n",
    "    for i in range(0, len(df.iloc[1])): \n",
    "        df = df[~(np.abs(df.iloc[:,i] - df.iloc[:,i].mean()) > (3*df.iloc[:,i].std()))]\n",
    "\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_std = std_based_outlier(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3_std.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_std.plot(kind='scatter', x='viewCount', y='editCount');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 가장 자주 쓰이는 방식\n",
    "# z-score 이용: 표준점수로 변환후 +-3 std 벗어나는 경우 제거\n",
    "from scipy import stats\n",
    "\n",
    "df3_zscore = df3[(np.abs(stats.zscore(df3)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_zscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "df3_zscore['viewCount'].plot(kind='box', ax=ax[0, 0], figsize=(5, 6));\n",
    "df3_zscore['editCount'].plot(kind='box', ax=ax[0, 1], figsize=(5, 6));\n",
    "df3_zscore['add'].plot(kind='box', ax=ax[1, 0], figsize=(5, 6));\n",
    "df3_zscore['searchCount'].plot(kind='box', ax=ax[1, 1], figsize=(5, 6));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_zscore.plot(kind='scatter', x='viewCount', y='editCount');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original std vs z-score std\n",
    "- 원 데이터로 처리시 매우 보수적인 결과 -> 효과가 낮아서 자주 쓰이지 않음\n",
    "- 일반적으로 z-score 자주 활용, 단 정규분포에 효과적\n",
    "- 정규분포가 아닐 경우, IQR 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR\n",
    "df3 = df2._get_numeric_data() \n",
    "\n",
    "for i in range(0, len(df3.iloc[1])): \n",
    "        \n",
    "    q1 = df3.iloc[:,i].quantile(0.25)\n",
    "    q3 = df3.iloc[:,i].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr       \n",
    "\n",
    "    df3 = df3[(df3.iloc[:,i] >= fence_low) & (df3.iloc[:,i] <= fence_high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최종 검토\n",
    "- IQR: 분포가 쏠려 있어서 (거의 median이 0), 조금만 벗어나도 이상치로 판정\n",
    "- 최종 선정은 zscore 기준으로 우선 선택!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_zscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max가 0인 컬럼은 제거\n",
    "cols_max = df3_zscore.describe().loc['max']\n",
    "drop_cols = cols_max[cols_max == 0]\n",
    "\n",
    "df3_zscore.drop(drop_cols.index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_zscore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "df3_zscore['viewCount'].plot(kind='box', ax=ax[0, 0], figsize=(5, 6));\n",
    "df3_zscore['editCount'].plot(kind='box', ax=ax[0, 1], figsize=(5, 6));\n",
    "df3_zscore['add'].plot(kind='box', ax=ax[1, 0], figsize=(5, 6));\n",
    "df3_zscore['searchCount'].plot(kind='box', ax=ax[1, 1], figsize=(5, 6));\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_zscore.plot(kind='scatter', x='viewCount', y='viewTraffic');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분포 변환\n",
    "- Transformation\n",
    "    - if right skewed: Log, Sqrt, cube root functions\n",
    "    - if left skwed: square\n",
    "- left_distribution: X^3\n",
    "- mild_left: X^2\n",
    "- mild_right: sqrt(X)\n",
    "- right: ln(X)\n",
    "- servere right: 1/X    \n",
    "    \n",
    "![img](skwed.png)\n",
    "\n",
    "- source: http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_03.pdf\n",
    "\n",
    "- Reference\n",
    "    - http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "    - http://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3_zscore.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분포를 간단히 확인 with IQR, MIN, MAX, MEAN, STD\n",
    "df4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier 미처리시 skewness 더 심해짐\n",
    "df4.hist(bins=15, color='darkblue', figsize=(18,14), grid=False); \n",
    "\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 함수 적용 (if right skewed)\n",
    "df4_log = df4.apply(lambda x: np.log(x+1))    \n",
    "\n",
    "df4_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_log.hist(bins=15, color='brown', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 함수 적용\n",
    "df4_cube_root = df4.apply(lambda x: x ** (1. / 3))    \n",
    "\n",
    "df4_cube_root.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_cube_root.hist(bins=15, color='brown', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q) 다양한 여러 함수를 적용해보시고 가장 정규분포로 잘 변환되는 함수를 찾아주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 우선 분포 변환은 Pass\n",
    "- 원 분포대로 모델링 구축하고 추후 개선시 transform 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단위 표준화 (Scaling)\n",
    "- 모든 변수의 단위를 동일한 기준(스케일)로 통일\n",
    "    - 이번 사례의 경우 tarffic(byte) 변수로 인해, 필수적인 과정\n",
    "- Standard Scaler (Mean: 0, std: 1)\n",
    "- MinMax Scaler (default: min=0, max=1)\n",
    "- Robust Scaler (x - q1 / q3-q1)\n",
    "- Source: http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "\n",
    "![img](scale_std.png)\n",
    "![img](scale_minmax.png)\n",
    "![img](scale_robust.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_std_scale = df4.copy() # after removes outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_std_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for c in df4_std_scale:\n",
    "    df4_std_scale[c] = StandardScaler().fit_transform(np.array(df4_std_scale[c]).reshape(-1, 1)).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_std_scale.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q) 단위 변화후 분포에는 어떠한 변화가 있을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.hist(bins=15, color='darkblue', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_std_scale.hist(bins=15, color='brown', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_minmax_scale = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for c in df4_minmax_scale:\n",
    "    df4_minmax_scale[c] = MinMaxScaler().fit_transform(np.array(df4_minmax_scale[c]).reshape(-1,1).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_minmax_scale.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_robust_scale = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "for c in df4_robust_scale:\n",
    "    df4_robust_scale[c] = RobustScaler().fit_transform(np.array(df4_robust_scale[c]).reshape(-1,1).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_robust_scale.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset after removes outliers, not transform, no scale\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.join(df2['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다중공선성 issues\n",
    "- 변수 삭제 1): EX) editTraffic, exportTraffic, viewTraffic, openCount\n",
    "- 변수 삭제 2): 분포가 정상이 아닌 경우 (우선 유지)\n",
    "- 변수 축소 EX) PCA, Factor Analysis, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(df5.corr(), annot=True, annot_kws={\"size\": 13}, cmap='Purples');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계 높은 변수 제거\n",
    "drop_cols = ['editTraffic', 'exportTraffic', 'viewTraffic', 'openCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalance Issues\n",
    "- 결제자 여부에 대한 데이터 사례가 불충분 => 모델이 sdu 로 대부분 예측하는 결과\n",
    "    - SDU(0)를 SDU(0)로 예측하는 정확도(True Negative)는 높을 수있으나, MDU(1)를 MDU(1) 예측하는 TP는 낮을 것으로 예상\n",
    "- how to handle imbalance    \n",
    "    - https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.group.value_counts().plot(kind='bar', color='darkblue', rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['group'] = np.where(df5['group'] == 'sdu', 0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 모델 종류\n",
    " - **Logistic Regression**\n",
    "     - Logistic regression fits a logistic model to data and makes predictions about the probability of an event (between 0 and \n",
    " - **Naive Bayes**\n",
    "    - Naive Bayes uses Bayes Theorem to model the conditional relationship of each attribute to the class variable\n",
    " - **k-Nearest Neighbor**\n",
    "    - The k-Nearest Neighbor (kNN) method makes predictions by locating similar cases to a given data instance (using a similarity function) and returning the average or majority of the most similar data instances. The kNN algorithm can be used for classification or regression.\n",
    " - **Trees-based model**\n",
    "    - Classification and Regression Trees (CART) are constructed from a dataset by making splits that best separate the data for the classes or predictions being made. The CART algorithm can be used for classification or regression\n",
    " - **Random Forest**\n",
    "    - Random Forest is a machine learning algorithm used for classification, regression, and feature selection. It's an ensemble technique, meaning it combines the output of one weaker technique in order to get a stronger result. The weaker technique in this case is a decision tree. Decision trees work by splitting the and re-splitting the data by features. If a decision tree is split along good features, it can give a decent predictive output    \n",
    " - **SVM (Support Vector Machines)**\n",
    "    - Support Vector Machines (SVM) are a method that uses points in a transformed problem space that best separate classes into two groups. Classification for multiple classes is supported by a one-vs-all method. SVM also supports regression by modeling the function with a minimum amount of allowable error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "- 모델 구축 후 성능 검증을 위해 전체 Dataset을 Train, Validation과 Test로 나눈다. \n",
    "- Testset은 최적화된 파라메터로 구축된 최종 모델의 성능을 파악하기 위해 단 1회만 사용한다. \n",
    "- 최적화 파라메터는 Scikit-learn에서 제공하는 grid_serach를 이용해 구한다.\n",
    "- Dataset을 나눌 때 test_size 옵션으로 Train, Test의 비율을 설정할 수 있고, random_state로 seed 값을 지정할 수 있다.\n",
    "- 데이터 샘플이 너무 많다면, 연상 비용이 크게 증가할 수 있어 샘플링이 필요하다.\n",
    "\n",
    "```python\n",
    "# 샘플링 예시 코드 / frac에는 샘플링셋의 비율을 입력, Replace는 비복원으로 지정(False)\n",
    "df_sampled = df.sample(frac=.1, replace=False) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# set ind vars and target var\n",
    "X = df5.drop('group', axis=1)\n",
    "y = df5.group\n",
    "\n",
    "# split train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit_transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max\n",
    "print(X_train_scaled.max(axis=0))\n",
    "print(X_test_scaled.max(axis=0))\n",
    "print(' ')\n",
    "# min\n",
    "print(X_train_scaled.min(axis=0))\n",
    "print(X_test_scaled.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 파라메터 설정\n",
    "- 기본 모델: **Logistic Regression** \n",
    "    - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- 주요 파라메터 (C)\n",
    "    - C 값 (기본값 = 1)\n",
    "    - C 값이 작으면 Penalty 강해짐 (단순 모델)\n",
    "    - C 값이 크면 Penalty 약해짐 (정규화 없어짐)\n",
    "    - 보통 로그스케일로 지정(10배씩) = 0.01, 0.1, 1, 10\n",
    "- penalty\n",
    "    - L2: Ridge, 일반적으로 사용 (default)\n",
    "    - L1: LASSO, 변수가 많아서 줄여야할 때 사용, 모델의 단순화 및 해석에 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set params\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "\n",
    "# fit\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How the grid_search module works:\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# SET default\n",
    "best_score = 0\n",
    "\n",
    "# iterataion\n",
    "for r in ['l1', 'l2']:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        lm = LogisticRegression(penalty = r, C=C)\n",
    "        scores = cross_val_score(lm, X_train, y_train, cv=5)\n",
    "        score = np.mean(scores)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'penalty': r}\n",
    "            \n",
    "```            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.score(X_test_scaled, y_test) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grid_search.predict(X_test_scaled)))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1차 모델 평가 (about the first model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('when grid searching: ', grid_search.best_score_)\n",
    "print('at the trainset:, ', grid_search.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 테스트셋의 label 분포\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 결과\n",
    "pd.Series(grid_search.predict(X_test_scaled)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(grid_search.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(grid_search.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC plot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, grid_search.predict(X_test))\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling & Downsampling for imbalanced data\n",
    "1. Collect More Data (if possible)\n",
    "2. Resampling the Dataset\n",
    "    - oversampling\n",
    "        - no information loss, perform better than undersampling\n",
    "        - overfitting issues (because of duplicates)\n",
    "    - undersampling\n",
    "        - help improve run time and storage problems\n",
    "        - information loss, biased dataset\n",
    "3. Generate Synthetic Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal dataset\n",
    "df5.group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.group.value_counts().transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(df):    \n",
    "\n",
    "    df_pay_only = df.query(\"group == 1\")\n",
    "    df_pay_only_over = pd.concat([df_pay_only, df_pay_only, df_pay_only], axis=0) \n",
    "    df_over = pd.concat([df, df_pay_only_over], axis=0)\n",
    "\n",
    "    return df_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over = oversampling(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over.group.value_counts().transform(lambda x: x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df5_over.drop(\"group\", axis=1)\n",
    "y = df5_over.group\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test_scaled.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set params\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "# grid search\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "\n",
    "# fit\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "#print(grid_search.best_estimator_)\n",
    "print(grid_search.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(grid_search.predict(X_test_scaled), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(grid_search.predict(X_test_scaled), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC plot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, grid_search.predict(X_test_scaled))\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The model performance of the first application\n",
    "- process\n",
    "    - oversampling\n",
    "    - dataset split\n",
    "    - minmax scale\n",
    "    - logistic regression, grid search, k-fold(5)\n",
    "    - evaluation\n",
    "    - Baseline score: **Precision: 0.5, Recall: 0.76, AUC: 0.74**\n",
    "- How to improve\n",
    "    - **There seems no overfitting issues**\n",
    "        - how to avoid overfitting: collect more data\n",
    "            - regularization\n",
    "            - feature deduction\n",
    "            - collect more samples\n",
    "    - **Feature Engineering**\n",
    "        - Other Scaling and Transformation\n",
    "        - Feature selection or creation\n",
    "            - Polynomial / Interactions\n",
    "            - new features\n",
    "        - Transformation\n",
    "            - log, exp, sqrt (if not tree-based model)\n",
    "            - Numeric to Categorical\n",
    "    - **Model, Parameter Tuning**\n",
    "        - KNN\n",
    "        - NB\n",
    "        - SVM\n",
    "        - RF\n",
    "        - NN .. any classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Improve\n",
    "- scale\n",
    "- distribution transformation\n",
    "- feature selection/polynomial or interation\n",
    "- apply other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Scale to z-score & pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over.head() # after removes outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipeline_logit(X_train, y_train):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    logit_model = LogisticRegression()\n",
    "\n",
    "    pipe = Pipeline([('scaler', scaler), ('model', logit_model)])\n",
    "\n",
    "    param_grid = [{'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  'model__penalty': ['l1', 'l2']}]\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = pipeline_logit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best score: \", grid_search.best_score_)\n",
    "print(\"best score: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(grid_search.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, grid_search.predict(X_test))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfrom Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over.hist(bins=15, color='darkblue', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over_log = df5_over.loc[:,:'traffic'].apply(lambda x: np.log(x + 1)).join(df5_over['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_over_log.hist(bins=15, color='brown', figsize=(18,14), grid=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df5_over_log.drop(\"group\", axis=1)\n",
    "y = df5_over_log.group\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = pipeline_logit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(grid, X_test, y_test):\n",
    "    \n",
    "    print(classification_report(grid.predict(X_test), y_test))\n",
    "\n",
    "    print(\"best score: \", grid.best_score_)\n",
    "    print(\"best params: \", grid.best_params_)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, grid.predict(X_test))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(grid_search, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC plot\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, grid_search.predict(X_test))\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The current score: **Precision: 0.77, Recall: 0.88, AUC: 0.86**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature Selection](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    " - Efficiency\n",
    " - [Multicollinearity](https://ko.wikipedia.org/wiki/%EB%8B%A4%EC%A4%91%EA%B3%B5%EC%84%A0%EC%84%B1)\n",
    " - How to select\n",
    "    - Univariate Selection: T-test, ANOVA, Coefficient \n",
    "    - Feature Importance (Tree-based model)\n",
    "    - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection\n",
    "- [F value](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "- 그룹내 분산이 작고, 그룹간 분산이 클 경우 F value가 커짐 (F value가 크다는 의미는 그룹간 통계적 차이가 크다는 것을 의미)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def pipeline_logit_kbest(X_train, y_train):\n",
    "\n",
    "    select = SelectKBest(score_func=f_classif) # if regression problem, score_func=f_regression\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    logit_model = LogisticRegression()\n",
    "\n",
    "    pipe = Pipeline([('scaler', scaler), ('feature_selection', select), ('model', logit_model)])\n",
    "\n",
    "    param_grid = [{'feature_selection__k': [3,5,7],\n",
    "                  'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  'model__penalty': ['l1', 'l2']\n",
    "                  }]\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_kbest = pipeline_logit_kbest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(grid_search_kbest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = grid_search_kbest.best_estimator_.named_steps['feature_selection'].get_support()\n",
    "features_list = list(X_train.columns.values)\n",
    "\n",
    "selected_features = []\n",
    "for bool, features in zip(mask, features_list):\n",
    "    if bool:\n",
    "        selected_features.append(features)\n",
    "        \n",
    "print(selected_features)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance of ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def pipeline_tree_kbest(X_train, y_train):\n",
    "\n",
    "    select = SelectKBest(score_func=f_classif) # if regression problem, score_func=f_regression\n",
    "\n",
    "#    scaler = StandardScaler()\n",
    "    extra_tree_model = ExtraTreesClassifier()\n",
    "\n",
    "    pipe = Pipeline([('feature_selection', select), ('model', extra_tree_model)])\n",
    "\n",
    "    param_grid = [{'feature_selection__k': [5,7],\n",
    "                   'model__max_depth': [4, 6], # max_depth: The maximum depth of the tree.\n",
    "                   'model__n_estimators': [10, 50], # n_estimators: The number of trees in the forest.\n",
    "                   'model__min_samples_split': [50, 100]}]\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_tree = pipeline_tree_kbest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(grid_search_tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = grid_search_tree.best_estimator_.named_steps['feature_selection'].get_support()\n",
    "feature_importance = grid_search_tree.best_estimator_.named_steps['model'].feature_importances_\n",
    "\n",
    "features_list = list(X_train.columns.values)\n",
    "\n",
    "selected_features = []\n",
    "for bool, features in zip(mask, features_list):\n",
    "    if bool:\n",
    "        selected_features.append(features)\n",
    "\n",
    "# create a df        \n",
    "feature_importance_pd = pd.DataFrame(list(zip(selected_features, feature_importance)),\\\n",
    "                                    columns=['features', 'importance'])\\\n",
    "                          .set_index(\"features\").sort_values(\"importance\")\n",
    "\n",
    "# visiual\n",
    "feature_importance_pd.plot(kind='barh', color='darkblue')\n",
    "\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.grid(color='lightgrey', alpha=0.5, linestyle='--')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q)  RFE (recursive feature elimination)\n",
    "- Backward 방식중 하나로 모든 변수를 다 포함시키고 반복해서 학습을 하면서 중요하지 않은 변수를 하나씩 제거하는 방식\n",
    "    - [API DOC](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)\n",
    "- 위 방식을 이용해서 원하는 모델을 이용해 Feature Selection(elimination)을 해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q) Random Forest, SVM이나 NB, Neural Network 등 다른 모델도 파이프라인에 사용해보세요.\n",
    "```python\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "params_grid = [{'n_neighbors': [3, 5, 10], # default: 5\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "                # cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’, ‘manhattan’\n",
    "               }]\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params_grid = [{'C': [1, 10], # Penalty parameter C of the error term\n",
    "                'gamma': [1, 10] # Higher the value of gamma, will try to exact fit\n",
    "                'kernel': ['linear', 'rbf']\n",
    "               }]\n",
    "\n",
    "# neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "params_grid = [{'solver': [1, 10],\n",
    "                'hidden_layer_sizes': [(5,2), (3,3)]\n",
    "               }]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
